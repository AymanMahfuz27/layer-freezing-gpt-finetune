{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AymanMahfuz27/layer-freezing-gpt-finetune/blob/main/efficient_finetuning_of_an_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aFKeYimKp7MX",
        "outputId": "dbab45d2-172b-4e02-9d60-c51b724349b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEfficient Fine-Tuning via Layer Freezing - Implementation\\nThis script implements the layer freezing approach for efficient fine-tuning\\nof transformer models as described in the research paper.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Efficient Fine-Tuning via Layer Freezing - Implementation\n",
        "This script implements the layer freezing approach for efficient fine-tuning\n",
        "of transformer models as described in the research paper.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_byvtBzTp7MZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Investigating Efficient Fine-Tuning of LLMs Through Selective Layer Adaptation\n",
        "\n",
        "## Abstract\n",
        "\n",
        "As the size of large language models (LLMs) continues to grow, full-parameter fine-tuning has become increasingly resource-intensive and impractical for many applications. Inspired by the hierarchical structure observed in deep convolutional networks, we hypothesize that LLMs also form layered abstractions — with lower layers specializing in syntactic patterns and higher layers specializing in semantic, task-specific reasoning. This work investigates whether fine-tuning only the uppermost layers of an LLM is sufficient for downstream task adaptation. We conduct controlled experiments comparing full fine-tuning to selective top-layer fine-tuning across a classification task. Our results suggest that significant training efficiency can be achieved with minimal degradation in performance, highlighting opportunities for lightweight model adaptation in resource-constrained settings.\n",
        "\n",
        "## Hypothesis\n",
        "\n",
        "Transformer-based language models exhibit hierarchical abstraction across their depth; therefore, fine-tuning only the final N layers of a pretrained LLM will yield comparable downstream performance to full fine-tuning while updating significantly fewer parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "5TanrThfQ2Nx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VoAitNdp7Ma"
      },
      "source": [
        "# Install required packages<br>\n",
        "import os<br>\n",
        "os.system(\"pip install transformers datasets torch evaluate tqdm matplotlib pandas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6-knkThp7Mb"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-nvtx-cu12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Kt8JaLrC1D",
        "outputId": "6d24c9ad-172a-4bd7-cc3e-4585bd6c1f5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "  Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "  Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "  Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "  Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "  Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "  Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "  Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "  Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "  Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "  Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.6.0+cu124 --extra-index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rxU8LI6rEFK",
        "outputId": "e683c4f7-f1b8-4f3d-8cf8-3ba9e8e10266"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch==2.6.0+cu124 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMUHJIocqbNd",
        "outputId": "767a56eb-8872-4b68-c751-5a4a5b447249"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.50.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "URqHEeh-p7Mc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EvalPrediction\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAJuIg7rp7Mc"
      },
      "source": [
        "Set seeds for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8s-bYjhAp7Md"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyHWufOWp7Md"
      },
      "source": [
        "Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoHXv_mtp7Me",
        "outputId": "8114d956-8f1c-4167-ece5-f07422b547e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_x0x4hhp7Me"
      },
      "source": [
        "Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BMCmEGwEp7Me"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gpt2\"\n",
        "TASK = \"boolq\"\n",
        "OUTPUT_DIR = \"./results\"\n",
        "MAX_LENGTH = 256  # increased for BoolQ context/questions\n",
        "BATCH_SIZE = 8    # reduce batch size due to increased max length (better for GPU memory)\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_EPOCHS = 3\n",
        "WEIGHT_DECAY = 0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alUX5QI9p7Mf",
        "outputId": "5a20570a-3939-4be4-8685-b5f8f898f717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gpt2\n",
            "Task: boolq\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Task: {TASK}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93xV26ZCp7Mf"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IdRdwBc1p7Mf"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_dataset(task_name):\n",
        "    \"\"\"Load dataset from Hugging Face and prepare it for training\"\"\"\n",
        "    print(f\"Loading dataset: {task_name}\")\n",
        "\n",
        "    # Load BoolQ directly from Hugging Face\n",
        "    if task_name == \"boolq\":\n",
        "        dataset = load_dataset(\"boolq\")\n",
        "        num_labels = 2\n",
        "        label_names = [\"False\", \"True\"]\n",
        "    elif task_name == \"mnli\":\n",
        "        dataset = load_dataset(\"mnli\")\n",
        "        num_labels = 3\n",
        "        label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "    else:\n",
        "        # Default for other tasks, or fallback to a known dataset\n",
        "        dataset = load_dataset(\"boolq\")\n",
        "        num_labels = 2\n",
        "        label_names = [\"False\", \"True\"]\n",
        "\n",
        "    print(f\"Dataset structure: {dataset}\")\n",
        "    return dataset, num_labels, label_names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDh5C5top7Mf"
      },
      "source": [
        "Load tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c9Qv6ZEap7Mf"
      },
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(model_name, num_labels):\n",
        "    \"\"\"Load pretrained model and tokenizer\"\"\"\n",
        "    print(f\"Loading tokenizer: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Handle models without pad token (like GPT-2)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    from transformers import GPT2ForSequenceClassification\n",
        "\n",
        "    model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    # GPT-2 needs special padding token handling\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_sciq_to_boolq(example):\n",
        "#     # Just pick correct answer randomly as true/false example clearly\n",
        "#     correct_answer = example['correct_answer']\n",
        "#     passage = example['support']\n",
        "#     question = example['question']\n",
        "#     return {\n",
        "#         \"question\": f\"{question}\",\n",
        "#         \"passage\": passage,\n",
        "#         \"label\": True  # Always true for correct answer passage\n",
        "#     }\n",
        "\n",
        "# # clearly preprocess SciQ\n",
        "# boolq_like_sciq = dataset.map(preprocess_sciq_to_boolq)\n",
        "# boolq_like_sciq = boolq_like_sciq[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "E6RmLgYuasFv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dZL9UbEp7Mg"
      },
      "source": [
        "Implement layer freezing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zLY6kHdTp7Mg"
      },
      "outputs": [],
      "source": [
        "def freeze_layers(model, num_layers_to_freeze):\n",
        "    \"\"\"\n",
        "    Freeze specific layers in GPT-2 explicitly and correctly.\n",
        "    \"\"\"\n",
        "    print(f\"Freezing {num_layers_to_freeze} GPT-2 layers...\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # GPT-2 transformer layers\n",
        "    if hasattr(model, \"transformer\") and hasattr(model.transformer, \"h\"):\n",
        "        layers_to_freeze = min(num_layers_to_freeze, len(model.transformer.h))\n",
        "        for i in range(layers_to_freeze):\n",
        "            for param in model.transformer.h[i].parameters():\n",
        "                param.requires_grad = False\n",
        "        print(f\"Froze {layers_to_freeze} GPT-2 transformer layers\")\n",
        "\n",
        "    # Freeze embeddings explicitly (optional but recommended)\n",
        "    for param in model.transformer.wte.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.transformer.wpe.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"Embeddings frozen\")\n",
        "\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    trainable_percentage = 100 * trainable_params / total_params\n",
        "\n",
        "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_percentage:.2f}% of total)\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    return model, trainable_percentage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IKp4iFop7Mg"
      },
      "source": [
        "Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "uYpd9kvTp7Mg"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(dataset, tokenizer, task_name):\n",
        "    \"\"\"Tokenize and prepare dataset for training\"\"\"\n",
        "    # Define tokenization function based on task\n",
        "    if task_name == \"boolq\":\n",
        "      def tokenize_function(examples):\n",
        "        questions = examples[\"question\"]\n",
        "        passages = examples[\"passage\"]\n",
        "        combined_input = [f\"Question: {q}\\nPassage: {p}\\nAnswer:\" for q, p in zip(questions, passages)]\n",
        "        tokenized = tokenizer(\n",
        "            combined_input,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH\n",
        "        )\n",
        "        # Convert boolean labels to int (0 or 1)\n",
        "        tokenized[\"labels\"] = [int(a) for a in examples[\"answer\"]]\n",
        "        return tokenized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif task_name == \"mnli\":\n",
        "        # Sentence pair classification\n",
        "        def tokenize_function(examples):\n",
        "            return tokenizer(\n",
        "                examples[\"premise\"],\n",
        "                examples[\"hypothesis\"],\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=MAX_LENGTH\n",
        "            )\n",
        "    else:\n",
        "        # Default approach for other tasks\n",
        "        def tokenize_function(examples):\n",
        "            # Check for sentence pairs\n",
        "            if \"sentence1\" in examples and \"sentence2\" in examples:\n",
        "                return tokenizer(\n",
        "                    examples[\"sentence1\"],\n",
        "                    examples[\"sentence2\"],\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    max_length=MAX_LENGTH\n",
        "                )\n",
        "            # Single sentence\n",
        "            elif \"sentence\" in examples:\n",
        "                return tokenizer(\n",
        "                    examples[\"sentence\"],\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    max_length=MAX_LENGTH\n",
        "                )\n",
        "            # Unknown format - try to use the first text field we find\n",
        "            else:\n",
        "                text_field = next(key for key in examples.keys() if isinstance(examples[key][0], str))\n",
        "                return tokenizer(\n",
        "                    examples[text_field],\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    max_length=MAX_LENGTH\n",
        "                )\n",
        "\n",
        "    print(\"Tokenizing datasets...\")\n",
        "    tokenized_datasets = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        desc=\"Tokenizing\",\n",
        "    )\n",
        "\n",
        "    # Set format for pytorch\n",
        "    # For each split in the tokenized dataset, remove all columns except the ones we need.\n",
        "    for split in tokenized_datasets.keys():\n",
        "        tokenized_datasets[split] = tokenized_datasets[split].remove_columns(\n",
        "            [col for col in tokenized_datasets[split].column_names if col not in [\"input_ids\", \"attention_mask\", \"labels\"]]\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    print(f\"Tokenized dataset structure: {tokenized_datasets}\")\n",
        "\n",
        "    return tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M6R9OXUp7Mh"
      },
      "source": [
        "Set up evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Z-6IO8eep7Mh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, predictions),\n",
        "        'f1': f1_score(labels, predictions)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6etL2LsCp7Mh"
      },
      "source": [
        "Fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "J5tTpFoUp7Mh"
      },
      "outputs": [],
      "source": [
        "def fine_tune_model(model, tokenized_datasets, tokenizer, output_dir, num_epochs=3):\n",
        "    \"\"\"Fine-tune the model with the tokenized dataset\"\"\"\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        report_to=\"none\",  # Disable reporting to save space in Colab\n",
        "        fp16=True,  # Enable mixed precision training\n",
        "        remove_unused_columns=False,  # We'll handle this ourselves\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    print(\"Starting fine-tuning...\")\n",
        "    train_result = trainer.train()\n",
        "    print(f\"Training completed. Results: {train_result.metrics}\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")\n",
        "    # Plot training loss\n",
        "   # Extract steps and loss values from log_history\n",
        "    steps = [entry[\"step\"] for entry in trainer.state.log_history if \"loss\" in entry]\n",
        "    losses = [entry[\"loss\"] for entry in trainer.state.log_history if \"loss\" in entry]\n",
        "\n",
        "    plt.plot(steps, losses)\n",
        "    plt.title(\"Training Loss Curve\")\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"loss_curve.png\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    return trainer, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gkytLN7p7Mi"
      },
      "source": [
        "Run experiments with different layer freezing configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "cY7uzVwLp7Mi"
      },
      "outputs": [],
      "source": [
        "def run_layer_freezing_experiments(model_name, dataset_name, layer_configs, num_epochs=2):\n",
        "    \"\"\"\n",
        "    Run experiments with different layer freezing configurations.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the pre-trained model\n",
        "        dataset_name: Name of the dataset to use\n",
        "        layer_configs: List of number of layers to freeze in each experiment\n",
        "        num_epochs: Number of epochs to train for each experiment\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Load dataset once\n",
        "    dataset, num_labels, label_names = load_and_prepare_dataset(dataset_name)\n",
        "    seeds = [42,52,62,72]\n",
        "    for layers_to_freeze in layer_configs:\n",
        "        for seed in seeds:\n",
        "            set_seed(seed)\n",
        "            experiment_name = f\"{model_name.split('/')[-1]}_{layers_to_freeze}_frozen_{seed}\"\n",
        "            print(f\"\\n===== Experiment: {experiment_name} =====\")\n",
        "\n",
        "            # Load fresh model for each experiment\n",
        "            model, tokenizer = load_model_and_tokenizer(model_name, num_labels)\n",
        "\n",
        "            # Apply freezing\n",
        "            model, trainable_pct = freeze_layers(model, layers_to_freeze)\n",
        "            print(f\"\\nStarting fine-tuning with {trainable_pct:.2f}% parameters trainable.\")\n",
        "\n",
        "\n",
        "            # Prepare dataset\n",
        "            tokenized_datasets = preprocess_dataset(dataset, tokenizer, dataset_name)\n",
        "\n",
        "            # Fine-tune\n",
        "            output_dir = os.path.join(OUTPUT_DIR, experiment_name)\n",
        "            trainer, eval_results = fine_tune_model(\n",
        "                model,\n",
        "                tokenized_datasets,\n",
        "                tokenizer,\n",
        "                output_dir,\n",
        "                num_epochs=num_epochs\n",
        "            )\n",
        "\n",
        "            # Record results\n",
        "            result = {\n",
        "                \"model\": model_name,\n",
        "                \"seeds\": seed,\n",
        "                \"layers_frozen\": layers_to_freeze,\n",
        "                \"trainable_params_pct\": trainable_pct,\n",
        "                \"accuracy\": float(eval_results.get(\"eval_accuracy\", 0)),\n",
        "                \"loss\": float(eval_results.get(\"eval_loss\", 0)),\n",
        "                \"training_time\": float(eval_results.get(\"eval_runtime\", 0)),\n",
        "            }\n",
        "\n",
        "\n",
        "            results.append(result)\n",
        "            print(f\"Experiment results: {result}\")\n",
        "\n",
        "    # Create and return results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\n===== Summary of All Experiments =====\")\n",
        "    print(results_df)\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_with_lora(model, tokenized_datasets, tokenizer, output_dir, num_epochs=3):\n",
        "    # Define training arguments for LoRA fine-tuning\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        report_to=\"none\",  # Disable reporting to save space in Colab\n",
        "        fp16=True,  # Enable mixed precision training\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        inference_mode=False,\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"c_attn\", \"c_proj\"]  # GPT-2 modules\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    train_result = trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    return trainer, eval_results\n"
      ],
      "metadata": {
        "id": "_XHiGW-IZCPv"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa23wf_Np7Mi"
      },
      "source": [
        "Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "VxAsCNzSp7Mi"
      },
      "outputs": [],
      "source": [
        "def plot_results(results_df):\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    grouped = results_df.groupby(\"layers_frozen\").agg({\n",
        "        \"accuracy\": [\"mean\", \"std\"],\n",
        "        \"trainable_params_pct\": \"first\"\n",
        "    }).reset_index()\n",
        "    grouped.columns = [\"layers_frozen\", \"accuracy_mean\", \"accuracy_std\", \"trainable_params_pct\"]\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Accuracy vs. Layers Frozen with error bars\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.errorbar(grouped[\"layers_frozen\"], grouped[\"accuracy_mean\"], yerr=grouped[\"accuracy_std\"], fmt='o-', capsize=5)\n",
        "    plt.xlabel(\"Layers Frozen\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy vs. Layers Frozen (mean ± std)\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Accuracy vs Trainable Parameters\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(grouped[\"trainable_params_pct\"], grouped[\"accuracy_mean\"], 'o-')\n",
        "    plt.xlabel(\"Trainable Parameters (%)\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Trainable Params vs Accuracy\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, \"layer_freezing_results.png\"))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih_3l4vKp7Mj"
      },
      "source": [
        "Function to test the model on sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "cNA4XfXKp7Mj"
      },
      "outputs": [],
      "source": [
        "def predict_sample_text(model, tokenizer, texts, label_names):\n",
        "    \"\"\"Make predictions on sample texts\"\"\"\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "        # Get prediction and confidence\n",
        "        pred_idx = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0, pred_idx].item()\n",
        "        prediction = label_names[pred_idx]\n",
        "\n",
        "        results.append({\n",
        "            \"text\": text,\n",
        "            \"prediction\": prediction,\n",
        "            \"confidence\": confidence\n",
        "        })\n",
        "\n",
        "    for result in results:\n",
        "        print(f\"Text: {result['text']}\")\n",
        "        print(f\"Prediction: {result['prediction']}\")\n",
        "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP0Rte9-p7Mj"
      },
      "source": [
        "Main execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "SAyYmrhmp7Mk"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the experiments\"\"\"\n",
        "    # Create output directory\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Load dataset\n",
        "    dataset, num_labels, label_names = load_and_prepare_dataset(\"boolq\")\n",
        "    # Use BoolQ train split and create a validation split if not provided\n",
        "    if \"validation\" not in dataset.keys():\n",
        "        dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "    else:\n",
        "        dataset = dataset\n",
        "\n",
        "    # Run a single fine-tuning experiment\n",
        "    print(\"\\n===== Running Single Model Fine-Tuning =====\")\n",
        "    model, tokenizer = load_model_and_tokenizer(MODEL_NAME, num_labels)\n",
        "\n",
        "    # Apply moderate layer freezing (adjust based on model size)\n",
        "    # For BERT-base which has 12 layers, freeze 6 layers\n",
        "    model, trainable_pct = freeze_layers(model, 8)\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    tokenized_datasets = preprocess_dataset(dataset, tokenizer, \"boolq\")\n",
        "\n",
        "    # Fine-tune the model\n",
        "    trainer, eval_results = fine_tune_model(model, tokenized_datasets, tokenizer, OUTPUT_DIR)\n",
        "\n",
        "    if \"test\" in dataset.keys():\n",
        "        eval_subset = dataset[\"test\"].select(range(100))\n",
        "    else:\n",
        "        eval_subset = dataset[\"validation\"].select(range(100))\n",
        "\n",
        "\n",
        "    example_texts = [\n",
        "        f\"Question: {x['question']}\\nPassage: {x['passage']}\\nAnswer:\" for x in eval_subset\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    predict_sample_text(model, tokenizer, example_texts, label_names)\n",
        "    print(\"\\n===== Running LoRA Fine-Tuning Baseline =====\")\n",
        "    model, tokenizer = load_model_and_tokenizer(MODEL_NAME, num_labels)\n",
        "    tokenized_datasets = preprocess_dataset(dataset, tokenizer, TASK)\n",
        "    trainer, eval_results = fine_tune_with_lora(model, tokenized_datasets, tokenizer, os.path.join(OUTPUT_DIR, \"lora_baseline\"))\n",
        "\n",
        "    print(f\"LoRA Fine-Tuning Results: {eval_results}\")\n",
        "\n",
        "    # Run experiments with different freezing configurations\n",
        "    print(\"\\n===== Running Layer Freezing Experiments =====\")\n",
        "    layer_configs = [0, 3, 6, 9]  # Try different freezing configurations\n",
        "    results_df = run_layer_freezing_experiments(\n",
        "        MODEL_NAME,\n",
        "        TASK,\n",
        "        layer_configs,\n",
        "        num_epochs=2  # Fewer epochs for experiments\n",
        "    )\n",
        "\n",
        "    # Save results to CSV\n",
        "    results_df.to_csv(os.path.join(OUTPUT_DIR, \"layer_freezing_results.csv\"), index=False)\n",
        "\n",
        "    # Plot results\n",
        "    plot_results(results_df)\n",
        "\n",
        "    print(\"\\n===== All experiments completed =====\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pkZiIDQp7Mk"
      },
      "source": [
        "Run the main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rO4f0IZqp7Mk",
        "outputId": "b5932b7e-201f-445d-bc30-08400989f626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset: boolq\n",
            "Dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer', 'passage'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['question', 'answer', 'passage'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "\n",
            "===== Running Single Model Fine-Tuning =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 8 GPT-2 layers...\n",
            "Froze 8 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 28,354,560 (22.79% of total)\n",
            "Total parameters: 124,441,344\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3537' max='3537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3537/3537 07:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.652600</td>\n",
              "      <td>0.634010</td>\n",
              "      <td>0.647095</td>\n",
              "      <td>0.761472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.615600</td>\n",
              "      <td>0.620928</td>\n",
              "      <td>0.648318</td>\n",
              "      <td>0.733673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.581000</td>\n",
              "      <td>0.629783</td>\n",
              "      <td>0.662080</td>\n",
              "      <td>0.748692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 420.5976, 'train_samples_per_second': 67.24, 'train_steps_per_second': 8.409, 'total_flos': 3694866701746176.0, 'train_loss': 0.6359391200330118, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6297826170921326, 'eval_accuracy': 0.6620795107033639, 'eval_f1': 0.7486922901978622, 'eval_runtime': 21.1254, 'eval_samples_per_second': 154.79, 'eval_steps_per_second': 19.361, 'epoch': 3.0}\n",
            "Text: Question: does ethanol take more energy make that produces\n",
            "Passage: All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6545\n",
            "--------------------------------------------------\n",
            "Text: Question: is house tax and property tax are same\n",
            "Passage: Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8460\n",
            "--------------------------------------------------\n",
            "Text: Question: is pain experienced in a missing body part or paralyzed area\n",
            "Passage: Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8751\n",
            "--------------------------------------------------\n",
            "Text: Question: is harry potter and the escape from gringotts a roller coaster ride\n",
            "Passage: Harry Potter and the Escape from Gringotts is an indoor steel roller coaster at Universal Studios Florida, a theme park located within the Universal Orlando Resort. Similar to dark rides, the roller coaster utilizes special effects in a controlled-lighting environment and also employs motion-based 3-D projection of both animation and live-action sequences to enhance the experience. The ride, which is themed to the Gringotts Wizarding Bank, became the flagship attraction for the expanded Wizarding World of Harry Potter when it opened on July 8, 2014.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8744\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a difference between hydroxyzine hcl and hydroxyzine pam\n",
            "Passage: Hydroxyzine preparations require a doctor's prescription. The drug is available in two formulations, the pamoate and the dihydrochloride or hydrochloride salts. Vistaril, Equipose, Masmoran, and Paxistil are preparations of the pamoate salt, while Atarax, Alamon, Aterax, Durrax, Tran-Q, Orgatrax, Quiess, and Tranquizine are of the hydrochloride salt.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7239\n",
            "--------------------------------------------------\n",
            "Text: Question: is barq's root beer a pepsi product\n",
            "Passage: Barq's /ˈbɑːrks/ is an American soft drink. Its brand of root beer is notable for having caffeine. Barq's, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq family but bottled by the Coca-Cola Company. It was known as Barq's Famous Olde Tyme Root Beer until 2012.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7348\n",
            "--------------------------------------------------\n",
            "Text: Question: can an odd number be divided by an even number\n",
            "Passage: In mathematics, parity is the property of an integer's inclusion in one of two categories: even or odd. An integer is even if it is evenly divisible by two and odd if it is not even. For example, 6 is even because there is no remainder when dividing it by 2. By contrast, 3, 5, 7, 21 leave a remainder of 1 when divided by 2. Examples of even numbers include −4, 0, 82 and 178. In particular, zero is an even number. Some examples of odd numbers are −5, 3, 29, and 73.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7180\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a word with q without u\n",
            "Passage: Of the 71 words in this list, 67 are nouns, and most would generally be considered loanwords; the only modern-English words that contain Q not followed by U and are not borrowed from another language are qiana, qwerty, and tranq. However, all of the loanwords on this list are considered to be naturalised in English according to at least one major dictionary (see References), often because they refer to concepts or societal roles that do not have an accurate equivalent in English. For words to appear here, they must appear in their own entry in a dictionary; words which occur only as part of a longer phrase are not included.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5300\n",
            "--------------------------------------------------\n",
            "Text: Question: can u drive in canada with us license\n",
            "Passage: Persons driving into Canada must have their vehicle's registration document and proof of insurance.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8598\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a play off for third place in the world cup\n",
            "Passage: The knockout stage of the 2018 FIFA World Cup was the second and final stage of the competition, following the group stage. It began on 30 June with the round of 16 and ended on 15 July with the final match, held at the Luzhniki Stadium in Moscow. The top two teams from each group (16 in total) advanced to the knockout stage to compete in a single-elimination style tournament. A third place play-off was also played between the two losing teams of the semi-finals.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9324\n",
            "--------------------------------------------------\n",
            "Text: Question: can minors drink with parents in new york\n",
            "Passage: In response to the National Minimum Drinking Age Act in 1984, which reduced by up to 10% the federal highway funding of any state which did not have a minimum purchasing age of 21, the New York Legislature raised the drinking age from 19 to 21, effective December 1, 1985. (The drinking age had been 18 for many years before the first raise on December 4th, 1982, to 19.) Persons under 21 are prohibited from purchasing alcohol or possessing alcohol with the intent to consume, unless the alcohol was given to that person by their parent or legal guardian. There is no law prohibiting where people under 21 may possess or consume alcohol that was given to them by their parents. Persons under 21 are prohibited from having a blood alcohol level of 0.02% or higher while driving.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7517\n",
            "--------------------------------------------------\n",
            "Text: Question: is the show bloodline based on a true story\n",
            "Passage: Bloodline was announced in October 2014 as part of a partnership between Netflix and Sony Pictures Television, representing Netflix's first major deal with a major film studio for a television series. The series was created and executive produced by Todd A. Kessler, Glenn Kessler, and Daniel Zelman, who previously created the FX series Damages. According to its official synopsis released by Netflix, Bloodline ``centers on a close-knit family of four adult siblings whose secrets and scars are revealed when their black sheep brother returns home.''\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.7650\n",
            "--------------------------------------------------\n",
            "Text: Question: is it bad to wash your hair with shower gel\n",
            "Passage: Shower gels for men may contain the ingredient menthol, which gives a cooling and stimulating sensation on the skin, and some men's shower gels are also designed specifically for use on hair and body. Shower gels contain milder surfactant bases than shampoos, and some also contain gentle conditioning agents in the formula. This means that shower gels can also double as an effective and perfectly acceptable substitute to shampoo, even if they are not labelled as a hair and body wash. Washing hair with shower gel should give approximately the same result as using a moisturising shampoo.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7607\n",
            "--------------------------------------------------\n",
            "Text: Question: is the liver part of the excretory system\n",
            "Passage: The liver detoxifies and breaks down chemicals, poisons and other toxins that enter the body. For example, the liver transforms ammonia (which is poisonous) into urea in fish, amphibians and mammals, and into uric acid in birds and reptiles. Urea is filtered by the kidney into urine or through the gills in fish and tadpoles. Uric acid is paste-like and expelled as a semi-solid waste (the ``white'' in bird excrements). The liver also produces bile, and the body uses bile to break down fats into usable fats and unusable waste.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6241\n",
            "--------------------------------------------------\n",
            "Text: Question: is fantastic beasts and where to find them a prequel\n",
            "Passage: Fantastic Beasts and Where to Find Them is a 2016 fantasy film directed by David Yates. A joint British and American production, it is a spin-off and prequel to the Harry Potter film series, and it was produced and written by J.K. Rowling in her screenwriting debut, and inspired by her 2001 book of the same name. The film stars Eddie Redmayne as Newt Scamander, with Katherine Waterston, Dan Fogler, Alison Sudol, Ezra Miller, Samantha Morton, Jon Voight, Carmen Ejogo, Ron Perlman, Colin Farrell and Johnny Depp in supporting roles. It is the first installment in the Fantastic Beasts film series, and ninth overall in the Wizarding World franchise, that began with the Harry Potter films.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8454\n",
            "--------------------------------------------------\n",
            "Text: Question: will there be a season 8 of vampire diaries\n",
            "Passage: The Vampire Diaries, an American supernatural drama, was renewed for an eighth season by The CW on March 11, 2016. On July 23, 2016, the CW announced that the upcoming season would be the series' last and would consist of 16 episodes. The season premiered on October 21, 2016 and concluded on March 10, 2017.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5507\n",
            "--------------------------------------------------\n",
            "Text: Question: was the movie strangers based on a true story\n",
            "Passage: The Strangers is a 2008 American slasher film written and directed by Bryan Bertino. Kristen (Liv Tyler) and James (Scott Speedman) are expecting a relaxing weekend at a family vacation home, but their stay turns out to be anything but peaceful as three masked torturers leave Kristen and James struggling for survival. Writer-director Bertino was inspired by real-life events: the Manson family Tate murders, a multiple homicide; the Keddie Cabin Murders, that occurred in California in 1981; and a series of break-ins that occurred in his own neighborhood as a child. Made on a budget of $9 million, the film was shot on location in rural South Carolina in the fall of 2006.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6502\n",
            "--------------------------------------------------\n",
            "Text: Question: is durham university part of the russell group\n",
            "Passage: In March 2012 it was announced that four universities -- Durham, Exeter, Queen Mary University of London; and York -- would become members of the Russell Group in August of the same year. All of the new members had previously been members of the 1994 Group of British universities.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6128\n",
            "--------------------------------------------------\n",
            "Text: Question: is the tv show the resident over for the season\n",
            "Passage: The Resident is an American medical drama television series aired by Fox Broadcasting Company that premiered on January 21, 2018, as a mid-season replacement entry in the 2017--18 television season. The fictional series focuses on the lives and duties of staff members at Chastain Park Memorial Hospital, while delving into the bureaucratic practices of the hospital industry. Formerly called The City, the show was purchased by Fox from Showtime in 2017. It was created by created by Amy Holden Jones, Hayley Schore, and Roshan Sethi. On May 10, 2017, Fox ordered a full 14-episode season and renewed the series for a second season on May 7, 2018. The first season officially concluded on May 14, 2018.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5292\n",
            "--------------------------------------------------\n",
            "Text: Question: does magnesium citrate have citric acid in it\n",
            "Passage: Magnesium citrate is a magnesium preparation in salt form with citric acid in a 1:1 ratio (1 magnesium atom per citrate molecule). The name ``magnesium citrate'' is ambiguous and sometimes may refer to other salts such as trimagnesium citrate which has a magnesium:citrate ratio of 3:2.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6230\n",
            "--------------------------------------------------\n",
            "Text: Question: does p o box come before street address\n",
            "Passage: Street Addressing will have the same street address of the post office, plus a ``unit number'' that matches the P.O. Box number. As an example, in El Centro, California, the post office is located at 1598 Main Street. Therefore, for P.O. Box 9975 (fictitious), the Street Addressing would be: 1598 Main Street Unit 9975, El Centro, CA. Nationally, the first five digits of the zip code may or may not be the same as the P.O. Box address, and the last four digits (Zip + 4) are virtually always different. Except for a few of the largest post offices in the U.S., the 'Street Addressing' (not the P.O. Box address) nine digit Zip + 4 is the same for all boxes at a given location.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6063\n",
            "--------------------------------------------------\n",
            "Text: Question: does a spark plug keep an engine running\n",
            "Passage: A spark plug (sometimes, in British English, a sparking plug, and, colloquially, a plug) is a device for delivering electric current from an ignition system to the combustion chamber of a spark-ignition engine to ignite the compressed fuel/air mixture by an electric spark, while containing combustion pressure within the engine. A spark plug has a metal threaded shell, electrically isolated from a central electrode by a porcelain insulator. The central electrode, which may contain a resistor, is connected by a heavily insulated wire to the output terminal of an ignition coil or magneto. The spark plug's metal shell is screwed into the engine's cylinder head and thus electrically grounded. The central electrode protrudes through the porcelain insulator into the combustion chamber, forming one or more spark gaps between the inner end of the central electrode and usually one or more protuberances or structures attached to the inner end of the threaded shell and designated the side, earth, or ground electrode(s).\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8189\n",
            "--------------------------------------------------\n",
            "Text: Question: is a cape and a cloak the same\n",
            "Passage: Ladies may wear a long (over the shoulders or to ankles) cloak usually called a cape, or a full-length cloak. Gentlemen wear an ankle-length or full-length cloak. Formal cloaks often have expensive, colored linings and trimmings such as silk, satin, velvet and fur.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5654\n",
            "--------------------------------------------------\n",
            "Text: Question: does it cost money to renounce us citizenship\n",
            "Passage: Renunciation of U.S. citizenship was free until July 2010, at which time a fee of $450 was established. An increase to $2,350, effective September 12, 2014, was justified as ``reflective of the true cost'' of processing. This followed a fee increase of approximately 220% in 2013. The increase took effect in January 2015.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5195\n",
            "--------------------------------------------------\n",
            "Text: Question: is a fire 7 the same as a kindle\n",
            "Passage: The Fire Tablet, formerly called the Kindle Fire, is a tablet computer developed by Amazon.com. Built with Quanta Computer, the Kindle Fire was first released in November 2011, featuring a color 7-inch multi-touch display with IPS technology and running a custom version of Google's Android operating system called Fire OS. The Kindle Fire HD followed in September 2012, and the Kindle Fire HDX in September 2013. In September 2014, when the fourth generation was introduced, the name ``Kindle'' was dropped. In September 2015, the fifth generation Fire 7 was released, followed by the sixth generation Fire HD 8, in September 2016. The seventh generation Fire 7 was released in June 2017.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6033\n",
            "--------------------------------------------------\n",
            "Text: Question: can you drink alcohol with your parents in wisconsin\n",
            "Passage: The drinking age in Wisconsin is 21. Those under the legal drinking age may be served, possess, or consume alcohol if they are with a parent, legal guardian, or spouse who is of legal drinking age. Those age 18-20 may also be served, possess or consumer alcohol if they are with a parent, legal guardian, or spouse who is of legal drinking age. Those age 18 to 20 may also possess (but not consume) alcohol as part of their employment.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8764\n",
            "--------------------------------------------------\n",
            "Text: Question: do penguins have feathers arising from the epidermis\n",
            "Passage: Contour feathers are not uniformly distributed on the skin of the bird except in some groups such as the penguins, ratites and screamers. In most birds the feathers grow from specific tracts of skin called pterylae; between the pterylae there are regions which are free of feathers called apterylae (or apteria). Filoplumes and down may arise from the apterylae. The arrangement of these feather tracts, pterylosis or pterylography, varies across bird families and has been used in the past as a means for determining the evolutionary relationships of bird families.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6268\n",
            "--------------------------------------------------\n",
            "Text: Question: do you need to break in a car\n",
            "Passage: A new engine is broken in by following specific driving guidelines during the first few hours of its use. The focus of breaking in an engine is on the contact between the piston rings of the engine and the cylinder wall. There is no universal preparation or set of instructions for breaking in an engine. Most importantly, experts disagree on whether it is better to start engines on high or low power to break them in. While there are still consequences to an unsuccessful break-in, they are harder to quantify on modern engines than on older models. In general, people no longer break in the engines of their own vehicles after purchasing a car or motorcycle, because the process is done in production. It is still common, even today, to find that an owner's manual recommends gentle use at first (often specified as the first 500 or 1000 kilometres or miles). But it is usually only normal use without excessive demands that is specified, as opposed to light/limited use. For example, the manual will specify that the car be driven normally, but not in excess of the highway speed limit.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6213\n",
            "--------------------------------------------------\n",
            "Text: Question: is the enchanted forest in oregon still open\n",
            "Passage: The Enchanted Forest is an amusement park located in Turner in the U.S. state of Oregon, next to Interstate 5 just south of Salem. Creator Roger Tofte opened the park in 1971 after seven years of construction. Today, the Tofte family still owns and operates the 20-acre (8.1 ha) park.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6217\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a golf course at the indy 500\n",
            "Passage: On the grounds of the speedway is the Indianapolis Motor Speedway Museum, which opened in 1956, and houses the Auto Racing Hall of Fame. The museum moved into its current building located in the infield in 1976. Also on the grounds is the Brickyard Crossing Golf Resort, which originally opened as the Speedway Golf Course in 1929. The golf course has 14 holes outside the track, along the backstretch, and four holes in the infield. The speedway also served as the venue for the opening ceremonies for the 1987 Pan American Games.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9123\n",
            "--------------------------------------------------\n",
            "Text: Question: does deadpool have a kid in the comics\n",
            "Passage: As part of Marvel's Marvel NOW! initiative a new Deadpool ongoing series was launched, written by Brian Posehn and Gerry Duggan and illustrated by Tony Moore. He is also a member of the Thunderbolts. In the 27th issue of his new series, as part of ``All-New Marvel NOW!'', Deadpool was married for the third time. Initially a secret, his bride was revealed in the webcomic Deadpool: The Gauntlet to be Shiklah, Queen of the Undead. Deadpool also discovers that he has a daughter by the name of Eleanor from a former flame of Deadpool named Carmelita.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8326\n",
            "--------------------------------------------------\n",
            "Text: Question: do they still make benson & hedges cigarettes\n",
            "Passage: Benson & Hedges is a British brand of cigarettes owned by either Philip Morris International, British American Tobacco, or Japan Tobacco, depending on the region. In the UK, they are registered in Old Bond Street in London, and are manufactured in Lisnafillan, Ballymena, Northern Ireland.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5207\n",
            "--------------------------------------------------\n",
            "Text: Question: is federal income tax the same as social security\n",
            "Passage: The Commonwealth government has its own tax laws and Puerto Ricans are also required to pay some US federal taxes, although most residents do not have to pay the federal personal income tax. In 2009, Puerto Rico paid $3.742 billion into the US Treasury. Residents of Puerto Rico pay into Social Security, and are thus eligible for Social Security benefits upon retirement. However, they are excluded from the Supplemental Security Income.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6311\n",
            "--------------------------------------------------\n",
            "Text: Question: is an engine speed sensor the same as a crankshaft sensor\n",
            "Passage: The crank sensor can be used in combination with a similar camshaft position sensor to monitor the relationship between the pistons and valves in the engine, which is particularly important in engines with variable valve timing. This method is also used to ``synchronise'' a four stroke engine upon starting, allowing the management system to know when to inject the fuel. It is also commonly used as the primary source for the measurement of engine speed in revolutions per minute.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5329\n",
            "--------------------------------------------------\n",
            "Text: Question: is indiana jones temple of doom a prequel\n",
            "Passage: Indiana Jones and the Temple of Doom is a 1984 American action-adventure film directed by Steven Spielberg. It is the second installment in the Indiana Jones franchise and a prequel to the 1981 film Raiders of the Lost Ark, featuring Harrison Ford reprising his role as the title character. After arriving in North India, Indiana Jones is asked by desperate villagers to find a mystical stone and rescue their children from a Thuggee cult practicing child slavery, black magic and ritual human sacrifice in honor of the goddess Kali.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6675\n",
            "--------------------------------------------------\n",
            "Text: Question: is there any next part of avengers infinity war\n",
            "Passage: The untitled Avengers film, colloquially referred to as Avengers 4, is an upcoming American superhero film based on the Marvel Comics superhero team the Avengers, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures. It is intended to be the direct sequel to 2018's Avengers: Infinity War, as well as the sequel to 2012's Marvel's The Avengers and 2015's Avengers: Age of Ultron and the twenty-second film in the Marvel Cinematic Universe (MCU). The film is directed by Anthony and Joe Russo, with a screenplay by the writing team of Christopher Markus and Stephen McFeely, and features an ensemble cast with many actors from previous MCU films.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9544\n",
            "--------------------------------------------------\n",
            "Text: Question: is the toyota highlander on a truck frame\n",
            "Passage: Announced in April 2000 at the New York Auto Show and arriving in late 2000 in Japan and January 2001 in North America, the Highlander became one of the first car-based mid-size SUV or mid-size crossovers. The Highlander is the crossover counterpart to the more rugged, truck-based midsize 4Runner and became Toyota's best-selling SUV before being surpassed by the smaller RAV4 in 2006. In Japan, the Kluger is exclusive to dealership network called Toyota NETZ as a larger alternative to the RAV4.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6797\n",
            "--------------------------------------------------\n",
            "Text: Question: is it legal to do a cover of a song\n",
            "Passage: Since the Copyright Act of 1909, United States musicians have had the right to record a version of someone else's previously recorded and released tune, whether it is music alone or music with lyrics. A license can be negotiated between representatives of the interpreting artist and the copyright holder, or recording published tunes can fall under a mechanical license whereby the recording artist pays a standard royalty to the original author/copyright holder through an organization such as the Harry Fox Agency, and is safe under copyright law even if they do not have any permission from the original author. A similar service was provided by Limelight by RightsFlow, until January 2015, when they announced they will be closing their service. The U.S. Congress introduced the mechanical license to head off an attempt by the Aeolian Company to monopolize the piano roll market.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7238\n",
            "--------------------------------------------------\n",
            "Text: Question: can carbon form polar covalent bonds with hydrogen\n",
            "Passage: The carbon-hydrogen bond (C--H bond) is a bond between carbon and hydrogen atoms that can be found in many organic compounds. This bond is a covalent bond meaning that carbon shares its outer valence electrons with up to four hydrogens. This completes both of their outer shells making them stable. Carbon--hydrogen bonds have a bond length of about 1.09 Å (1.09 × 10 m) and a bond energy of about 413 kJ/mol (see table below). Using Pauling's scale--C (2.55) and H (2.2)--the electronegativity difference between these two atoms is 0.35. Because of this small difference in electronegativities, the C−H bond is generally regarded as being non-polar. In structural formulas of molecules, the hydrogen atoms are often omitted. Compound classes consisting solely of C--H bonds and C--C bonds are alkanes, alkenes, alkynes, and aromatic hydrocarbons. Collectively they are known as hydrocarbons.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8785\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a sequel to the movie the golden compass\n",
            "Passage: In 2011, Philip Pullman remarked at the British Humanist Association annual conference that due to the first film's disappointing sales in the United States, there would not be any sequels made.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.9784\n",
            "--------------------------------------------------\n",
            "Text: Question: is columbus day a national holiday in the united states\n",
            "Passage: Columbus Day is a national holiday in many countries of the Americas and elsewhere which officially celebrates the anniversary of Christopher Columbus's arrival in the Americas on October 12, 1492. The landing is celebrated as ``Columbus Day'' in the United States, as ``Día de la Raza'' (``Day of the Race'') in some countries in Latin America, as ``Día de la Hispanidad'' and ``Fiesta Nacional'' in Spain, where it is also the religious festivity of la Virgen del Pilar, as Día de las Américas (Day of the Americas) in Belize and Uruguay, as Día del Respeto a la Diversidad Cultural (Day of Respect for Cultural Diversity) in Argentina, and as Giornata Nazionale di Cristoforo Colombo or Festa Nazionale di Cristoforo Colombo in Italy as well as in Little Italys around the world. As the day of remembrance of Our Lady of the Pillar, 12 October had been declared a religious feast day throughout the Spanish Empire in 1730; the secular Fiesta de la Raza Española was first proposed by Faustino Rodríguez-San Pedro y Díaz-Argüelles in 1913. In recent years, celebration of the holiday has faced some opposition from various organizations.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7998\n",
            "--------------------------------------------------\n",
            "Text: Question: are new balance and nike the same company\n",
            "Passage: New Balance maintains a manufacturing presence in the United States, as well as in the United Kingdom for the European market, where they produce some of their most popular models such as the 990 model--in contrast to its competitors, which often manufacture exclusively outside the USA and Europe. As a result, New Balance shoes tend to be more expensive than those of many other manufacturers. To offset this pricing difference, New Balance claims to differentiate their products with technical features, such as blended gel inserts, heel counters and a greater selection of sizes, particularly for very narrow and/or very wide widths. The company has made total profits of approximately $69 billion since 1992. They are the second most-renown American sporting company, after Nike.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.7840\n",
            "--------------------------------------------------\n",
            "Text: Question: is there an interstate that goes coast to coast\n",
            "Passage: U.S. Highway 20 (US 20) is an east--west United States highway that stretches from the Pacific Northwest all the way to New England. The ``0'' in its route number indicates that US 20 is a coast-to-coast route. Spanning 3,365 miles (5,415 km), it is the longest road in the United States, and particularly from Idaho to Massachusetts, the route roughly parallels that of Interstate 90 (I-90), which is in turn the longest Interstate Highway in the U.S. There is a discontinuity in the official designation of US 20 through Yellowstone National Park, with unnumbered roads used to traverse the park.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7709\n",
            "--------------------------------------------------\n",
            "Text: Question: is pureed tomatoes the same as tomato sauce\n",
            "Passage: Tomato purée is a thick liquid made by cooking and straining tomatoes. The difference between tomato paste, tomato purée, and tomato sauce is consistency; tomato puree has a thicker consistency and a deeper flavour than sauce.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.9064\n",
            "--------------------------------------------------\n",
            "Text: Question: can there be a word without a vowel\n",
            "Passage: English orthography typically represents vowel sounds with the five conventional vowel letters ⟨a, e, i, o, u⟩, as well as ⟨y⟩, which may also be a consonant depending on context. However, outside of abbreviations, there are a handful of words in English that do not have vowels, either because the vowel sounds are not written with vowel letters or because the words themselves are pronounced without vowel sounds.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7265\n",
            "--------------------------------------------------\n",
            "Text: Question: does only the winner get money on tipping point\n",
            "Passage: Tipping Point is a British television game show which began airing on ITV on 2 July 2012, and is presented by Ben Shephard. Four contestants answer general knowledge questions to win counters which they use on a large coin pusher arcade-style machine. Only the winner at the end has a chance to take home any money; the others leave with nothing except any non-cash prizes they may have won during the game.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5594\n",
            "--------------------------------------------------\n",
            "Text: Question: is there such a thing as a turkey vulture\n",
            "Passage: The turkey vulture (Cathartes aura), also known in some North American regions as the turkey buzzard (or just buzzard), and in some areas of the Caribbean as the John crow or carrion crow, is the most widespread of the New World vultures. One of three species in the genus Cathartes of the family Cathartidae, the turkey vulture ranges from southern Canada to the southernmost tip of South America. It inhabits a variety of open and semi-open areas, including subtropical forests, shrublands, pastures, and deserts.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9498\n",
            "--------------------------------------------------\n",
            "Text: Question: has anyone hit a hole in one on a par 5\n",
            "Passage: As of October 2008, a condor (four under par) hole-in-one on a par 5 hole had been recorded on four occasions, aided by thin air at high altitude, or by cutting the corner on a doglegged or horseshoe-shaped hole. A horseshoe-shaped par 5 hole once enabled a condor hole in one to be achieved with a 3-iron club. The longest recorded straight drive hole-in-one is believed to be 517 yards or 473 metres, on the par 5 No. 9 hole at Green Valley Ranch Golf Club in Denver in 2002, aided by the thin air due to the high altitude. None of these four par 5 holes-in-one were achieved during a professional tournament. A condor is also known as a double albatross, or a triple eagle.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9139\n",
            "--------------------------------------------------\n",
            "Text: Question: do the jets and giants share a stadium\n",
            "Passage: MetLife Stadium is an American sports stadium located in East Rutherford, New Jersey, 8 miles outside of New York City. It is part of the Meadowlands Sports Complex and serves as the home stadium for two National Football League (NFL) franchises: the New York Giants and the New York Jets. The stadium is owned by the MetLife Stadium Company, a joint venture of the Giants and Jets, who jointly built the stadium using private funds on land owned by the New Jersey Sports and Exposition Authority. The stadium opened as New Meadowlands Stadium in 2010. In 2011, MetLife, an insurance company based in New York City, acquired the naming rights to the stadium. At a construction cost of approximately $1.6 billion, it was the most expensive stadium ever built, at the time it opened, and is the second-largest stadium in the NFL in terms of seating capacity.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7776\n",
            "--------------------------------------------------\n",
            "Text: Question: is the us womens soccer team in the world cup\n",
            "Passage: After the defeat in the 2016 Olympics, the USWNT underwent a year of experimentation which saw them losing 3 home games. If not for a comeback win against Brazil, the USWNT was on the brink of losing 4 home games in one year, a low never before seen by the USWNT. 2017 saw the USWNT play 12 games against teams ranked in the top-15 in the world. The USWNT heads into World Cup Qualifying in fall of 2018.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.7262\n",
            "--------------------------------------------------\n",
            "Text: Question: can an african team win the world cup\n",
            "Passage: Association football is the most popular sport in nearly every African country, and 13 members of the Confederation of African Football (CAF) have competed at the sport's biggest event -- the men's FIFA World Cup.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8947\n",
            "--------------------------------------------------\n",
            "Text: Question: can a hammer be used as a weapon\n",
            "Passage: Many martial arts employ the use of common objects as weapons; Filipino martial arts such as Eskrima include practice with machetes, canes, bamboo spears, and knives as a result of the 333 year Spanish colonization that took place in the Philippines which prohibited the ownership and use of standard swords and bladed weapons; Chinese martial arts and some Korean martial arts commonly feature the use of improvised weapons such as fans, hammers and staves. There are even some western martial arts that are based on improvised weapons such as British quarterstaff fighting and Irish stick fighting.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8871\n",
            "--------------------------------------------------\n",
            "Text: Question: do they still have fox hunts in england\n",
            "Passage: Fox hunting with hounds, as a formalised activity, originated in England in the sixteenth century, in a form very similar to that practised until February 2005, when a law banning the activity in England and Wales came into force. A ban on hunting in Scotland had been passed in 2002, but it continues to be within the law in Northern Ireland and several other countries, including Australia, Canada, France, Ireland and the United States. In Australia, the term also refers to the hunting of foxes with firearms, similar to deer hunting. In much of the world, hunting in general is understood to relate to any game animals or weapons (e.g., deer hunting with bow and arrow); in Britain and Ireland, ``hunting'' without qualification implies fox hunting (or other forms of hunting with hounds--beagling, drag hunting, hunting the clean boot, mink hunting, or stag hunting), as described here.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6812\n",
            "--------------------------------------------------\n",
            "Text: Question: can you wear short sleeve shirt with asu jacket\n",
            "Passage: The ASU includes a midnight blue coat and low waist trousers for male soldiers; and a midnight blue coat, slacks and skirt for female soldiers. The fabric for the ASU is heavier and more wrinkle resistant than previously manufactured uniforms and will consist of 55% wool and 45% polyester material. The ASU coat has a tailored, athletic cut to improve uniform fit and appearance. The ASU includes an improved heavier and wrinkle resistant short and long-sleeved white shirt with permanent military creases and shoulder loops. The JROTC version replaces the white shirt with the prototype grey shirt and gold braid is not worn on the blue trousers or on the sleeves of the class A coat. Compared to the Army's previous uniforms, the ASU does not include a garrison cap; soldiers will continue to wear the Army's berets.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5627\n",
            "--------------------------------------------------\n",
            "Text: Question: has wisconsin ever been in the little league world series\n",
            "Passage: This is the list of U.S. states that have participated in the Little League World Series. As of the 2018 LLWS, eight states had never reached the LLWS: Alaska, Colorado, Kansas, North Dakota, Utah, Vermont, Wisconsin, and Wyoming; additionally, the District of Columbia has never reached the LLWS.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6323\n",
            "--------------------------------------------------\n",
            "Text: Question: does damon and elena get together in season 3\n",
            "Passage: In the third season, Damon helps Elena in bringing his brother, Stefan, back to Mystic Falls after Stefan becomes Klaus' henchman. The arrangement transpired after a bargain for his blood that would cure Damon of the werewolf bite he had received from Tyler. At first, he is reluctant to involve Elena in the rescue attempts, employing Alaric Saltzman, Elena's guardian, instead as Klaus does not know that Elena is alive after the sacrifice which frees Klaus' hybrid side. However, Elena involves herself, desperate to find Stefan. Damon, though hesitant at first, is unable to refuse her because of his love for her. He also points out to her that she once turned back from finding Stefan since she knew Damon would be in danger, clearly showing that she also has feelings for him. He tells her that ``when (he) drag(s) (his) brother from the edge to deliver him back to (her), (he) wants her to remember the things (she) felt while he was gone.'' When Stefan finally returns to Mystic Falls, his attitude is different from that of the first and second seasons. This causes a rift between Elena and Stefan whereas the relationship between Damon and Elena becomes closer and more intimate. A still loyal Elena, however, refuses to admit her feelings for Damon. In 'Dangerous Liaisons', Elena, frustrated with her feelings for him, tells Damon that his love for her may be a problem, and that this could be causing all their troubles. This incenses Damon, causing him to revert to the uncaring and reckless Damon seen in the previous seasons. The rocky relationship between the two continues until the sexual tension hits the fan and in a moment of heated passion, Elena -- for the first time in the three seasons -- kisses Damon of her own accord. This kiss finally causes Elena to admit that she loves both brothers and realize that she must ultimately make her choice as her own ancestress, Katherine Pierce, who turned the brothers, once did. In assessment of her feelings for Damon, she states this: ``Damon just sort of snuck up on me. He got under my skin and no matter what I do, I can't shake him.'' In the season finale, a trip designed to get her to safety forces Elena to make her choice: to go to Damon and possibly see him one last time; or to go to Stefan and her friends and see them one last time. She chooses the latter when she calls Damon to tell him her decision. Damon, who is trying to stop Alaric, accepts what she says and she tells him that maybe if she had met Damon before she had met Stefan, her choice may have been different. This statement causes Damon to remember the first night he did meet Elena which was, in fact, the night her parents died - before she had met Stefan. Not wanting anyone to know he was in town and after giving her some advice about life and love, Damon compels her to forget. He remembers this as he fights Alaric and seems accepting of his death when Alaric, whose life line is tied to Elena's, suddenly collapses in his arms. Damon is grief-stricken, knowing that this means that Elena has also died and yells, ``No! You are not dead!'' A heartbroken Damon then goes to the hospital demanding to see Elena when the doctor, Meredith Fell, tells him that she gave Elena vampire blood. The last shot of the season finale episode shows Elena in transition.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5368\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a player in the nfl missing a hand\n",
            "Passage: Shaquem Alphonso Griffin /ʃəˈkiːm/ (born July 20, 1995) is an American football linebacker for the Seattle Seahawks of the National Football League (NFL). He is the twin brother of Seahawks cornerback Shaquill Griffin, and both brothers played college football for the University of Central Florida Knights. As an amputee with one hand, Shaquem Griffin received extensive media coverage as a prospective 2018 NFL Draft pick. He was selected as a fifth round pick (141st overall) by the Seahawks on April 28, 2018, reuniting him with Shaquill.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9323\n",
            "--------------------------------------------------\n",
            "Text: Question: is the other boleyn girl part of a series\n",
            "Passage: The novel was followed by a sequel called The Queen's Fool, set during the reign of Henry's daughter, Queen Mary. The Queen's Fool was followed by The Virgin's Lover, set during the early days of Queen Elizabeth I's reign.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7573\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a group called the five heartbeats\n",
            "Passage: To promote the film prior to its release, Townsend, along with the other actors who portrayed the fictional musical quartet The Five Heartbeats (Leon Robinson, Michael Wright, Harry J. Lennix, and Tico Wells) performed in a concert with real-life Soul/R&B vocal group The Dells, one of many groups that inspired the film. The Dells sang and recorded the vocals as the actors lip synced.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7958\n",
            "--------------------------------------------------\n",
            "Text: Question: is mount everest a part of the himalayas\n",
            "Passage: The Himalayan range has many of the Earth's highest peaks, including the highest, Mount Everest. The Himalayas include over fifty mountains exceeding 7,200 metres (23,600 ft) in elevation, including ten of the fourteen 8,000-metre peaks. By contrast, the highest peak outside Asia (Aconcagua, in the Andes) is 6,961 metres (22,838 ft) tall.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5894\n",
            "--------------------------------------------------\n",
            "Text: Question: can an emt-basic start an iv\n",
            "Passage: EMT-I/85 is a level of EMT-I training formulated by the National Registry of Emergency Medical Technicians in 1985. This training level includes more invasive procedures than are covered at the EMT-Basic level, including IV therapy, the use of advanced airway devices, and provides for advanced assessment skills. The EMT-I/85 typically administered the same medications as an EMT-B (oxygen, oral glucose, activated charcoal, epinephrine auto-injectors (EpiPens), nitroglycerin, and metered-dose inhalers such as albuterol). However, in some states they were also allowed to administer naloxone, D50, and glucagon. Like all other EMT levels, their scope of practice was governed by the state and/or their Medical Director.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9093\n",
            "--------------------------------------------------\n",
            "Text: Question: has no 1 court at wimbledon got a roof\n",
            "Passage: In April 2013, the All England Club confirmed its intention to build a retractable roof over No.1 Court. The roof is expected to be in place for the 2019 Championships.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5120\n",
            "--------------------------------------------------\n",
            "Text: Question: has anyone come back from 3-0 in the nba finals\n",
            "Passage: The following is the list of teams to overcome 3--1 series deficits by winning three straight games to win a best-of-seven playoff series. In the history of major North American pro sports, teams that were down 3--1 in the series came back and won the series 52 times, more than half of them were accomplished by National Hockey League (NHL) teams. Teams overcame 3--1 deficit in the final championship round eight times, six were accomplished by Major League Baseball (MLB) teams in the World Series. Teams overcoming 3--0 deficit by winning four straight games were accomplished five times, four times in the NHL and once in MLB.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9419\n",
            "--------------------------------------------------\n",
            "Text: Question: do radio waves travel at the speed of light\n",
            "Passage: Radio waves are a type of electromagnetic radiation with wavelengths in the electromagnetic spectrum longer than infrared light. Radio waves have frequencies as high as 300 gigahertz (GHz) to as low as 30 hertz (Hz). At 300 GHz, the corresponding wavelength is 1 mm, and at 30 Hz is 10,000 km. Like all other electromagnetic waves, radio waves travel at the speed of light. They are generated by electric charges undergoing acceleration, such as time varying electric currents. Naturally occurring radio waves are emitted by lightning and astronomical objects.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7089\n",
            "--------------------------------------------------\n",
            "Text: Question: did anyone from the 1980 us hockey team play in the nhl\n",
            "Passage: Of the 20 players on Team USA, 13 eventually played in the NHL. Five of them went on to play over 500 NHL games, and three would play over 1,000 NHL games.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8742\n",
            "--------------------------------------------------\n",
            "Text: Question: do all triangles have at least two acute angles\n",
            "Passage: An acute triangle is a triangle with all three angles acute (less than 90°). An obtuse triangle is one with one obtuse angle (greater than 90°) and two acute angles. Since a triangle's angles must sum to 180°, no triangle can have more than one obtuse angle.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6867\n",
            "--------------------------------------------------\n",
            "Text: Question: is baylor and mary hardin baylor the same school\n",
            "Passage: The University of Mary Hardin--Baylor (UMHB) is a Christian co-educational institution of higher learning located in Belton, Texas, United States. UMHB was chartered by the Republic of Texas in 1845 as Baylor Female College, the female department of what is now Baylor University. It has since become its own institution and grown to 3,914 students and awards degrees at the baccalaureate, master's, and doctoral levels. It is affiliated with the Baptist General Convention of Texas.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5154\n",
            "--------------------------------------------------\n",
            "Text: Question: can you get the death penalty as a minor\n",
            "Passage: Capital punishment for juveniles in the United States existed until March 1, 2005, when the U.S. Supreme Court banned it in Roper v. Simmons.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6961\n",
            "--------------------------------------------------\n",
            "Text: Question: did indian football team qualified for fifa 2018\n",
            "Passage: India has never participated in the FIFA World Cup, although the team did qualify by default for the 1950 World Cup after all the other nations in their qualification group withdrew. However, India withdrew prior to the beginning of the tournament. The team has also appeared three times in the Asia's top football competition, the AFC Asian Cup. Their best result in the competition occurred in 1964 when the team finished as runners-up. India also participate in the SAFF Championship, the top regional football competition in South Asia. They have won the tournament six times since it began in 1993.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.8852\n",
            "--------------------------------------------------\n",
            "Text: Question: are t rex and tyrannosaurus rex the same\n",
            "Passage: Tyrannosaurus is a genus of coelurosaurian theropod dinosaur. The species Tyrannosaurus rex (rex meaning ``king'' in Latin), often colloquially called simply T. rex or T-Rex, is one of the most well-represented of the large theropods. Tyrannosaurus lived throughout what is now western North America, on what was then an island continent known as Laramidia. Tyrannosaurus had a much wider range than other tyrannosaurids. Fossils are found in a variety of rock formations dating to the Maastrichtian age of the upper Cretaceous Period, 68 to 66 million years ago. It was the last known member of the tyrannosaurids, and among the last non-avian dinosaurs to exist before the Cretaceous--Paleogene extinction event.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6400\n",
            "--------------------------------------------------\n",
            "Text: Question: is the old panama canal still in use\n",
            "Passage: The new locks opened for commercial traffic on 26 June 2016, and the first ship to cross the canal using the third set of locks was a modern New Panamax vessel, the Chinese-owned container ship Cosco Shipping Panama. The original locks, now over 100 years old, allow engineers greater access for maintenance, and are projected to continue operating indefinitely.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6479\n",
            "--------------------------------------------------\n",
            "Text: Question: do you need a pal to possess ammunition\n",
            "Passage: A possession and acquisition licence is a licence that allows individuals in Canada to possess and acquire firearms as well as ammunition. Licences are typically valid for five years and must be renewed prior to expiry to maintain all classes. If an individual possessing a PAL is convicted of certain offences, a PAL can be revoked. If an individual does not renew their PAL prior to its expiration date or if they have their PAL revoked, they must legally dispose of any firearms in their possession. A licence for prohibited firearms can be issued to qualifying businesses, and very rarely to individuals (firearms they own, as the gun laws changed over time.) Previous convictions for serious violent, drug or weapons offences almost invariably result in the denial of the application.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5732\n",
            "--------------------------------------------------\n",
            "Text: Question: do blue and pink cotton candy taste the same\n",
            "Passage: Typically, once spun, cotton candy is only marketed by color. Absent a clear name other than 'blue', the distinctive taste of the blue raspberry flavor mix has gone on to become a compound flavor that some other foods (gum, ice cream, rock candy, fluoride toothpaste) occasionally borrow (``cotton-candy flavored ice cream'') to invoke the nostalgia of cotton candy that people typically only get to experience on vacation or holidays. Pink bubble gum went through a similar transition from specific branded product to a generic flavor that transcended the original confection, and 'bubble gum flavor' often shows up in the same product categories as 'cotton candy flavor'.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5792\n",
            "--------------------------------------------------\n",
            "Text: Question: did to kill a mockingbird win an academy award\n",
            "Passage: The film received overwhelmingly positive reviews from critics and was a box-office success, earning more than six times its budget. The film won three Academy Awards, including Best Actor for Peck, and was nominated for eight, including Best Picture.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5697\n",
            "--------------------------------------------------\n",
            "Text: Question: is there such a thing as a floating island\n",
            "Passage: A floating island is a mass of floating aquatic plants, mud, and peat ranging in thickness from several centimetres to a few metres. Floating islands are a common natural phenomenon that are found in many parts of the world. They exist less commonly as a man-made phenomenon. Floating islands are generally found on marshlands, lakes, and similar wetland locations, and can be many hectares in size.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9000\n",
            "--------------------------------------------------\n",
            "Text: Question: do female ferrets die if they don't mate\n",
            "Passage: Males, if not neutered, are extremely musky. It is considered preferable to delay neutering until sexual maturity has been reached, at approximately six to eight months old, after the full descent of the testicles. Neutering the male will reduce the smell to almost nothing. The same applies for females, but spaying them is also important for their own health. Unless they are going to be used for breeding purposes, female ferrets will go into extended heat. A female that does not mate can die of aplastic anemia without medical intervention. It is possible to use a vasectomised male to take a female out of heat.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6149\n",
            "--------------------------------------------------\n",
            "Text: Question: will all xbox 360 games work on xbox one\n",
            "Passage: The Xbox One gaming console has received updates from Microsoft since its launch in 2013 that enable it to play select games from its two predecessor consoles, Xbox and Xbox 360. On June 15, 2015, backward compatibility with supported Xbox 360 games became available to eligible Xbox Preview program users with a beta update to the Xbox One system software. The dashboard update containing backward compatibility was released publicly on November 12, 2015. On October 24, 2017, another such update added games from the original Xbox library. The following is a list of all backward compatible games on Xbox One under this functionality.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9564\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a right and left brachiocephalic artery\n",
            "Passage: There is no brachiocephalic artery for the left side of the body. The left common carotid, and the left subclavian artery, come directly off the aortic arch. However, there are two brachiocephalic veins.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.7346\n",
            "--------------------------------------------------\n",
            "Text: Question: do the runners up on survivor win money\n",
            "Passage: The Sole Survivor receives a cash prize of $1,000,000 prior to taxes and sometimes also receives a car provided by the show's sponsor. Every player receives a prize for participating on Survivor depending on how long he or she lasts in the game. In most seasons, the runner-up receives $100,000, and third place wins $85,000. All other players receive money on a sliding scale, though specific amounts have rarely been made public. Sonja Christopher, the first player voted off of Survivor: Borneo, received $2,500. In Survivor: Fiji, the first season with tied runners-up, the two runners-up received US$100,000 each, and Yau-Man Chan received US$60,000 for his fourth-place finish. All players also receive an additional $10,000 for their appearance on the reunion show.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7435\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a sequel to love finds a home\n",
            "Passage: Love Finds a Home is a Christian drama film, the eighth and final installment based on a series of books by Janette Oke. It aired on Hallmark Channel on September 5, 2009. The film is based on the book Love Finds a Home by Janette Oke. Sarah Jones, Haylie Duff, and Jordan Bridges reprise their roles from Love Takes Wing.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8568\n",
            "--------------------------------------------------\n",
            "Text: Question: will there be a second season of 11.22.63\n",
            "Passage: When asked about developing a sequel series, King stated ``I'd love to revisit Jake and Sadie, and also revisit the rabbit hole that dumps people into the past, but sometimes it's best not to go back for a second helping.''.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6921\n",
            "--------------------------------------------------\n",
            "Text: Question: are there nuclear power plants in the us\n",
            "Passage: Nuclear power in the United States is provided by 99 commercial reactors with a net capacity of 100,350 megawatts (MW), 65 pressurized water reactors and 34 boiling water reactors. In 2016 they produced a total of 805.3 terawatt-hours of electricity, which accounted for 19.7% of the nation's total electric energy generation. In 2016, nuclear energy comprised nearly 60 percent of U.S. emission-free generation.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8104\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a tiebreaker in final set at wimbledon\n",
            "Passage: The tiebreak is sometimes not employed for the final set of a match and an advantage set is used instead. Therefore, the deciding set must be played until one player or team has won two more games than the opponent. This is true in three of the four major tennis championships, all except the US Open where a tiebreak is played even in the deciding set (fifth set for the men, third set for the women) at 6--6. A tiebreak is not played in the deciding set in the other three majors -- the Australian Open, the French Open, and Wimbledon. (When the tiebreak was first introduced at Wimbledon in 1971, it was invoked at 8--8 rather than 6--6.) The US Open holds ``Super Saturday'' where the two men's semi-finals are played along with the women's final on the second Saturday of the event; therefore a tie-break is more prudent where player rest and scheduling is more important.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5263\n",
            "--------------------------------------------------\n",
            "Text: Question: were the twin towers the world trade center\n",
            "Passage: The original World Trade Center was a large complex of seven buildings in Lower Manhattan, New York City, United States. It featured the landmark Twin Towers, which opened on April 4, 1973 and were destroyed in 2001 during the September 11 attacks. At the time of their completion, the Twin Towers -- the original 1 World Trade Center, at 1,368 feet (417 m); and 2 World Trade Center, at 1,362 feet (415.1 m) -- were the tallest buildings in the world. Other buildings in the complex included the Marriott World Trade Center (3 WTC), 4 WTC, 5 WTC, 6 WTC, and 7 WTC. The complex was located in New York City's Financial District and contained 13,400,000 square feet (1,240,000 m) of office space.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7717\n",
            "--------------------------------------------------\n",
            "Text: Question: did deion sanders ever win a world series\n",
            "Passage: Sanders played football primarily at cornerback, but also as a kick returner, punt returner, and occasionally wide receiver. He played in the National Football League (NFL) for the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Ravens, winning the Super Bowl with both the 49ers and the Cowboys. An outfielder in baseball, he played professionally for the New York Yankees, the Atlanta Braves, the Cincinnati Reds and the San Francisco Giants, and participated in the 1992 World Series with the Braves. He attended Florida State University, where he was recognized as a two-time All-American in football, and also played baseball and ran track.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8498\n",
            "--------------------------------------------------\n",
            "Text: Question: is a german shepard the same as an alsatian\n",
            "Passage: The German Shepherd (German: Deutscher Schäferhund, German pronunciation: (ˈʃɛːfɐˌhʊnt)) is a breed of medium to large-sized working dog that originated in Germany. The breed's officially recognized name is German Shepherd Dog in the English language (sometimes abbreviated as GSD). The breed is known as the Alsatian in Britain and Ireland. The German Shepherd is a relatively new breed of dog, with their origin dating to 1899. As part of the Herding Group, German Shepherds are working dogs developed originally for herding sheep. Since that time however, because of their strength, intelligence, trainability, and obedience, German Shepherds around the world are often the preferred breed for many types of work, including disability assistance, search-and-rescue, police and military roles, and even acting. The German Shepherd is the second-most registered breed by the American Kennel Club and seventh-most registered breed by The Kennel Club in the United Kingdom.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8555\n",
            "--------------------------------------------------\n",
            "Text: Question: does a frog jump out of boiling water\n",
            "Passage: While some 19th-century experiments suggested that the underlying premise is true if the heating is sufficiently gradual, according to contemporary biologists the premise is false: a frog that is gradually heated will jump out. Indeed, thermoregulation by changing location is a fundamentally necessary survival strategy for frogs and other ectotherms.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.5738\n",
            "--------------------------------------------------\n",
            "Text: Question: is it possible to create mass from energy\n",
            "Passage: In high-energy particle colliders, matter creation events have yielded a wide variety of exotic heavy particles precipitating out of colliding photon jets (see two-photon physics). Currently, two-photon physics studies creation of various fermion pairs both theoretically and experimentally (using particle accelerators, air showers, radioactive isotopes, etc.).\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.9208\n",
            "--------------------------------------------------\n",
            "Text: Question: is there a movie with 0 on rotten tomatoes\n",
            "Passage: On the film review aggregation website Rotten Tomatoes, films that all surveyed critics consider bad have a 0% rating. Some of these are often considered some of the worst films ever made.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6278\n",
            "--------------------------------------------------\n",
            "Text: Question: is the jaguar s type rear wheel drive\n",
            "Passage: From model years 1999 to 2002, the rear-wheel-drive S-Type was equipped with either a five-speed manual (Getrag 221) or a five-speed J-Gate Ford 5R55N transmission . From 2003, the S-Type was produced with either a 5-speed manual transmission or a six-speed J-Gate transmission that allows automatic gear selection or clutchless manual gear selection. The 2004 diesel saw the introduction of a 6-speed manual transmission; it was also available with the six-speed J-Gate automatic transmission.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7149\n",
            "--------------------------------------------------\n",
            "Text: Question: is a tablespoon bigger than a dessert spoon\n",
            "Passage: As a unit of culinary measure, a level dessertspoon (dstspn.) equals two teaspoons, or 10 milliliters, whereas a U.S. tablespoon is three teaspoons (15ml or half a fluid ounce) in the U.S., and two dessertspoons, i.e. four teaspoons (20ml or two thirds of a fluid ounce) in Britain and Australia, which is the old British standard. For dry ingredients, a rounded or heaped teaspoonful is often specified instead.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.6868\n",
            "--------------------------------------------------\n",
            "Text: Question: is this the last season of bunk'd\n",
            "Passage: The series was renewed for a third season by Disney Channel on August 31, 2017. On June 1, 2018, it was announced that Peyton List, Karan Brar, Skai Jackson, and Miranda May would be returning for the third season and that Raphael Alejandro, Will Buie Jr., and Mallory Mahoney would be joining the cast. The third season premiered on Disney Channel on June 18, 2018. In March 2018, actress Skai Jackson stated in an interview that she was leaving Disney and that Bunk'd would end with the third season.\n",
            "Answer:\n",
            "Prediction: False\n",
            "Confidence: 0.7289\n",
            "--------------------------------------------------\n",
            "Text: Question: does the president live in the white house\n",
            "Passage: The White House is the official residence and workplace of the President of the United States. It is located at 1600 Pennsylvania Avenue NW in Washington, D.C. and has been the residence of every U.S. President since John Adams in 1800. The term is often used as a metonym for the president and his advisers.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5397\n",
            "--------------------------------------------------\n",
            "Text: Question: does the dorsal root ganglion carry sensory input\n",
            "Passage: A dorsal root ganglion (or spinal ganglion) (also known as a posterior root ganglion), is a cluster of neurons (a ganglion) in a dorsal root of a spinal nerve. The cell bodies of sensory neurons known as first-order neurons are located in the dorsal root ganglia.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8289\n",
            "--------------------------------------------------\n",
            "Text: Question: is anne with an e filmed on pei\n",
            "Passage: The series is filmed partially in Prince Edward Island as well as locations in Southern Ontario (including Millbrook and Caledon).\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.7580\n",
            "--------------------------------------------------\n",
            "Text: Question: is angular frequency and angular velocity the same\n",
            "Passage: Angular frequency (or angular speed) is the magnitude of the vector quantity angular velocity. The term angular frequency vector ω → (\\displaystyle (\\vec (\\omega ))) is sometimes used as a synonym for the vector quantity angular velocity.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6060\n",
            "--------------------------------------------------\n",
            "Text: Question: can someone die from a bullet shot in the air\n",
            "Passage: Bullets fired into the air usually fall back with terminal velocities much lower than their muzzle velocity when they leave the barrel of a firearm. Nevertheless, people can be injured, sometimes fatally, when bullets discharged into the air fall back down to the ground. Bullets fired at angles less than vertical are more dangerous as the bullet maintains its angular ballistic trajectory and is far less likely to engage in tumbling motion; it therefore travels at speeds much higher than a bullet in free fall.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8007\n",
            "--------------------------------------------------\n",
            "Text: Question: is salt lake city the biggest city in utah\n",
            "Passage: Salt Lake City (often shortened to Salt Lake and abbreviated as SLC) is the capital and the most populous municipality of the U.S. state of Utah. With an estimated population of 190,884 in 2014, the city is the core of the Salt Lake City metropolitan area, which has a population of 1,153,340 (2014 estimate). Salt Lake City is further situated within a larger metropolis known as the Salt Lake City--Ogden--Provo Combined Statistical Area. This region is a corridor of contiguous urban and suburban development stretched along an approximately 120-mile (190 km) segment of the Wasatch Front, comprising a population of 2,423,912 as of 2014. It is one of only two major urban areas in the Great Basin (the other is Reno, Nevada).\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.5087\n",
            "--------------------------------------------------\n",
            "Text: Question: was chasing cars written for grey's anatomy\n",
            "Passage: ``Chasing Cars'' is a song by Northern Irish alternative rock band Snow Patrol. It was released as the second single from their fourth studio album, Eyes Open (2006). It was recorded in 2005 and released on 6 June 2006 in the United States and 24 July 2006 in the United Kingdom. The song gained significant popularity in the US after being featured in the second season finale of the popular medical drama Grey's Anatomy, which aired on 15 May 2006.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.8103\n",
            "--------------------------------------------------\n",
            "Text: Question: did the girl in the lost world die\n",
            "Passage: On Isla Sorna, an island off the Pacific coast of Costa Rica, a young girl named Cathy Bowman wanders around during a family vacation, and survives an attack by a swarm of Compsognathus. Her parents file a lawsuit against the genetics company InGen, now headed by John Hammond's nephew, Peter Ludlow, who plans to use Isla Sorna to alleviate the financial losses imposed by the incident that occurred at Jurassic Park four years earlier. Mathematician Dr. Ian Malcolm meets Hammond at his mansion. Hammond explains that Isla Sorna, abandoned years earlier during a hurricane, is where InGen created their dinosaurs before moving them to Jurassic Park on Isla Nublar. Hammond hopes to stop InGen by sending a team to Isla Sorna to document the dinosaurs, thus causing public support against human interference on the island. Ian, who survived the Jurassic Park disaster, is reluctant. After learning that his girlfriend, paleontologist Dr. Sarah Harding, is part of the team and is already on Isla Sorna, Ian agrees to go to the island, but only to retrieve her.\n",
            "Answer:\n",
            "Prediction: True\n",
            "Confidence: 0.6120\n",
            "--------------------------------------------------\n",
            "\n",
            "===== Running LoRA Fine-Tuning Baseline =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n",
            "<ipython-input-76-949460c24d34>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3537' max='3537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3537/3537 10:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.663100</td>\n",
              "      <td>0.655978</td>\n",
              "      <td>0.620183</td>\n",
              "      <td>0.763429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.647500</td>\n",
              "      <td>0.658936</td>\n",
              "      <td>0.611009</td>\n",
              "      <td>0.709456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.639300</td>\n",
              "      <td>0.647821</td>\n",
              "      <td>0.617431</td>\n",
              "      <td>0.721319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:24]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Fine-Tuning Results: {'eval_loss': 0.655978262424469, 'eval_accuracy': 0.6201834862385321, 'eval_f1': 0.7634285714285715, 'eval_runtime': 25.0619, 'eval_samples_per_second': 130.477, 'eval_steps_per_second': 16.32, 'epoch': 3.0}\n",
            "\n",
            "===== Running Layer Freezing Experiments =====\n",
            "Loading dataset: boolq\n",
            "Dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer', 'passage'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['question', 'answer', 'passage'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "\n",
            "===== Experiment: gpt2_0_frozen_42 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 0 GPT-2 layers...\n",
            "Froze 0 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 85,057,536 (68.35% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 68.35% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 07:58, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.655600</td>\n",
              "      <td>0.637900</td>\n",
              "      <td>0.660245</td>\n",
              "      <td>0.779081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.577500</td>\n",
              "      <td>0.606364</td>\n",
              "      <td>0.685015</td>\n",
              "      <td>0.757875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 478.3253, 'train_samples_per_second': 39.417, 'train_steps_per_second': 4.93, 'total_flos': 2463244467830784.0, 'train_loss': 0.6651641353901612, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6063637733459473, 'eval_accuracy': 0.6850152905198776, 'eval_f1': 0.7578749412317819, 'eval_runtime': 21.3123, 'eval_samples_per_second': 153.432, 'eval_steps_per_second': 19.191, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 42, 'layers_frozen': 0, 'trainable_params_pct': 68.35150864330106, 'accuracy': 0.6850152905198776, 'loss': 0.6063637733459473, 'training_time': 21.3123}\n",
            "\n",
            "===== Experiment: gpt2_0_frozen_52 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 0 GPT-2 layers...\n",
            "Froze 0 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 85,057,536 (68.35% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 68.35% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 08:02, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.655400</td>\n",
              "      <td>0.649788</td>\n",
              "      <td>0.645872</td>\n",
              "      <td>0.774269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.584100</td>\n",
              "      <td>0.616565</td>\n",
              "      <td>0.674006</td>\n",
              "      <td>0.745342</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 482.4312, 'train_samples_per_second': 39.081, 'train_steps_per_second': 4.888, 'total_flos': 2463244467830784.0, 'train_loss': 0.6333162341712388, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6165650486946106, 'eval_accuracy': 0.674006116207951, 'eval_f1': 0.7453416149068323, 'eval_runtime': 21.2913, 'eval_samples_per_second': 153.584, 'eval_steps_per_second': 19.21, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 52, 'layers_frozen': 0, 'trainable_params_pct': 68.35150864330106, 'accuracy': 0.674006116207951, 'loss': 0.6165650486946106, 'training_time': 21.2913}\n",
            "\n",
            "===== Experiment: gpt2_0_frozen_62 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 0 GPT-2 layers...\n",
            "Froze 0 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 85,057,536 (68.35% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 68.35% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 07:48, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.650700</td>\n",
              "      <td>0.645054</td>\n",
              "      <td>0.645566</td>\n",
              "      <td>0.773854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.585700</td>\n",
              "      <td>0.613543</td>\n",
              "      <td>0.669113</td>\n",
              "      <td>0.744450</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 468.4066, 'train_samples_per_second': 40.251, 'train_steps_per_second': 5.034, 'total_flos': 2463244467830784.0, 'train_loss': 0.6330325229376226, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6135426163673401, 'eval_accuracy': 0.6691131498470948, 'eval_f1': 0.7444496929617384, 'eval_runtime': 21.382, 'eval_samples_per_second': 152.933, 'eval_steps_per_second': 19.128, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 62, 'layers_frozen': 0, 'trainable_params_pct': 68.35150864330106, 'accuracy': 0.6691131498470948, 'loss': 0.6135426163673401, 'training_time': 21.382}\n",
            "\n",
            "===== Experiment: gpt2_0_frozen_72 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 0 GPT-2 layers...\n",
            "Froze 0 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 85,057,536 (68.35% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 68.35% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 08:06, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.659900</td>\n",
              "      <td>0.640261</td>\n",
              "      <td>0.644037</td>\n",
              "      <td>0.773453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.596600</td>\n",
              "      <td>0.605362</td>\n",
              "      <td>0.677982</td>\n",
              "      <td>0.757205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 486.9838, 'train_samples_per_second': 38.716, 'train_steps_per_second': 4.842, 'total_flos': 2463244467830784.0, 'train_loss': 0.6357178109697208, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.605362057685852, 'eval_accuracy': 0.6779816513761467, 'eval_f1': 0.7572054415494581, 'eval_runtime': 21.4114, 'eval_samples_per_second': 152.722, 'eval_steps_per_second': 19.102, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 72, 'layers_frozen': 0, 'trainable_params_pct': 68.35150864330106, 'accuracy': 0.6779816513761467, 'loss': 0.605362057685852, 'training_time': 21.4114}\n",
            "\n",
            "===== Experiment: gpt2_3_frozen_42 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 3 GPT-2 layers...\n",
            "Froze 3 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 63,793,920 (51.26% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 51.26% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 06:39, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.660600</td>\n",
              "      <td>0.639381</td>\n",
              "      <td>0.662080</td>\n",
              "      <td>0.778335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.597000</td>\n",
              "      <td>0.611104</td>\n",
              "      <td>0.683792</td>\n",
              "      <td>0.758411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 399.6372, 'train_samples_per_second': 47.178, 'train_steps_per_second': 5.9, 'total_flos': 2463244467830784.0, 'train_loss': 0.6679752081304005, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6111035943031311, 'eval_accuracy': 0.6837920489296636, 'eval_f1': 0.758411214953271, 'eval_runtime': 21.4242, 'eval_samples_per_second': 152.631, 'eval_steps_per_second': 19.091, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 42, 'layers_frozen': 3, 'trainable_params_pct': 51.26424864070899, 'accuracy': 0.6837920489296636, 'loss': 0.6111035943031311, 'training_time': 21.4242}\n",
            "\n",
            "===== Experiment: gpt2_3_frozen_52 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 3 GPT-2 layers...\n",
            "Froze 3 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 63,793,920 (51.26% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 51.26% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 06:57, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.648000</td>\n",
              "      <td>0.649817</td>\n",
              "      <td>0.657798</td>\n",
              "      <td>0.776334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.590800</td>\n",
              "      <td>0.616664</td>\n",
              "      <td>0.687768</td>\n",
              "      <td>0.767691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 417.1236, 'train_samples_per_second': 45.2, 'train_steps_per_second': 5.653, 'total_flos': 2463244467830784.0, 'train_loss': 0.6349867480199554, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6166643500328064, 'eval_accuracy': 0.6877675840978593, 'eval_f1': 0.7676905574516496, 'eval_runtime': 21.5062, 'eval_samples_per_second': 152.05, 'eval_steps_per_second': 19.018, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 52, 'layers_frozen': 3, 'trainable_params_pct': 51.26424864070899, 'accuracy': 0.6877675840978593, 'loss': 0.6166643500328064, 'training_time': 21.5062}\n",
            "\n",
            "===== Experiment: gpt2_3_frozen_62 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 3 GPT-2 layers...\n",
            "Froze 3 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 63,793,920 (51.26% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 51.26% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 06:39, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.648400</td>\n",
              "      <td>0.623283</td>\n",
              "      <td>0.673089</td>\n",
              "      <td>0.775184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.577200</td>\n",
              "      <td>0.608307</td>\n",
              "      <td>0.677064</td>\n",
              "      <td>0.751295</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 399.2633, 'train_samples_per_second': 47.222, 'train_steps_per_second': 5.906, 'total_flos': 2463244467830784.0, 'train_loss': 0.6292530930173711, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6083067059516907, 'eval_accuracy': 0.6770642201834862, 'eval_f1': 0.7512953367875648, 'eval_runtime': 21.3394, 'eval_samples_per_second': 153.237, 'eval_steps_per_second': 19.166, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 62, 'layers_frozen': 3, 'trainable_params_pct': 51.26424864070899, 'accuracy': 0.6770642201834862, 'loss': 0.6083067059516907, 'training_time': 21.3394}\n",
            "\n",
            "===== Experiment: gpt2_3_frozen_72 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 3 GPT-2 layers...\n",
            "Froze 3 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 63,793,920 (51.26% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 51.26% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 06:44, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.637100</td>\n",
              "      <td>0.616341</td>\n",
              "      <td>0.667890</td>\n",
              "      <td>0.764425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.585000</td>\n",
              "      <td>0.613144</td>\n",
              "      <td>0.674312</td>\n",
              "      <td>0.748524</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 404.6905, 'train_samples_per_second': 46.589, 'train_steps_per_second': 5.827, 'total_flos': 2463244467830784.0, 'train_loss': 0.6231994564195524, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6131442189216614, 'eval_accuracy': 0.6743119266055045, 'eval_f1': 0.7485242030696576, 'eval_runtime': 21.5231, 'eval_samples_per_second': 151.929, 'eval_steps_per_second': 19.003, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 72, 'layers_frozen': 3, 'trainable_params_pct': 51.26424864070899, 'accuracy': 0.6743119266055045, 'loss': 0.6131442189216614, 'training_time': 21.5231}\n",
            "\n",
            "===== Experiment: gpt2_6_frozen_42 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 6 GPT-2 layers...\n",
            "Froze 6 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 42,530,304 (34.18% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 34.18% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 05:30, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.654500</td>\n",
              "      <td>0.625922</td>\n",
              "      <td>0.666972</td>\n",
              "      <td>0.778072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.603000</td>\n",
              "      <td>0.621884</td>\n",
              "      <td>0.666055</td>\n",
              "      <td>0.738756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 330.7262, 'train_samples_per_second': 57.008, 'train_steps_per_second': 7.13, 'total_flos': 2463244467830784.0, 'train_loss': 0.6743621373196796, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6259219646453857, 'eval_accuracy': 0.6669724770642201, 'eval_f1': 0.7780721418381903, 'eval_runtime': 21.3244, 'eval_samples_per_second': 153.345, 'eval_steps_per_second': 19.18, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 42, 'layers_frozen': 6, 'trainable_params_pct': 34.176988638116924, 'accuracy': 0.6669724770642201, 'loss': 0.6259219646453857, 'training_time': 21.3244}\n",
            "\n",
            "===== Experiment: gpt2_6_frozen_52 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 6 GPT-2 layers...\n",
            "Froze 6 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 42,530,304 (34.18% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 34.18% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 05:32, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.647200</td>\n",
              "      <td>0.628073</td>\n",
              "      <td>0.664832</td>\n",
              "      <td>0.769166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.591400</td>\n",
              "      <td>0.623879</td>\n",
              "      <td>0.671865</td>\n",
              "      <td>0.752251</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 332.7416, 'train_samples_per_second': 56.663, 'train_steps_per_second': 7.087, 'total_flos': 2463244467830784.0, 'train_loss': 0.6362680268550546, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6238793134689331, 'eval_accuracy': 0.6718654434250765, 'eval_f1': 0.7522512121911799, 'eval_runtime': 21.2947, 'eval_samples_per_second': 153.56, 'eval_steps_per_second': 19.207, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 52, 'layers_frozen': 6, 'trainable_params_pct': 34.176988638116924, 'accuracy': 0.6718654434250765, 'loss': 0.6238793134689331, 'training_time': 21.2947}\n",
            "\n",
            "===== Experiment: gpt2_6_frozen_62 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 6 GPT-2 layers...\n",
            "Froze 6 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 42,530,304 (34.18% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 34.18% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 05:31, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.639300</td>\n",
              "      <td>0.643954</td>\n",
              "      <td>0.666972</td>\n",
              "      <td>0.778253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.584700</td>\n",
              "      <td>0.611707</td>\n",
              "      <td>0.680122</td>\n",
              "      <td>0.764096</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 331.2607, 'train_samples_per_second': 56.916, 'train_steps_per_second': 7.118, 'total_flos': 2463244467830784.0, 'train_loss': 0.6296575544646881, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6117072701454163, 'eval_accuracy': 0.6801223241590214, 'eval_f1': 0.7640956247180875, 'eval_runtime': 21.4785, 'eval_samples_per_second': 152.245, 'eval_steps_per_second': 19.042, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 62, 'layers_frozen': 6, 'trainable_params_pct': 34.176988638116924, 'accuracy': 0.6801223241590214, 'loss': 0.6117072701454163, 'training_time': 21.4785}\n",
            "\n",
            "===== Experiment: gpt2_6_frozen_72 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 6 GPT-2 layers...\n",
            "Froze 6 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 42,530,304 (34.18% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 34.18% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 05:24, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.644000</td>\n",
              "      <td>0.621774</td>\n",
              "      <td>0.665749</td>\n",
              "      <td>0.772907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.587900</td>\n",
              "      <td>0.614346</td>\n",
              "      <td>0.675229</td>\n",
              "      <td>0.755074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 324.6678, 'train_samples_per_second': 58.072, 'train_steps_per_second': 7.263, 'total_flos': 2463244467830784.0, 'train_loss': 0.6282497338059597, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6143456697463989, 'eval_accuracy': 0.6752293577981652, 'eval_f1': 0.7550738007380073, 'eval_runtime': 21.2906, 'eval_samples_per_second': 153.589, 'eval_steps_per_second': 19.21, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 72, 'layers_frozen': 6, 'trainable_params_pct': 34.176988638116924, 'accuracy': 0.6752293577981652, 'loss': 0.6143456697463989, 'training_time': 21.2906}\n",
            "\n",
            "===== Experiment: gpt2_9_frozen_42 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 9 GPT-2 layers...\n",
            "Froze 9 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 21,266,688 (17.09% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 17.09% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 04:25, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.661000</td>\n",
              "      <td>0.639134</td>\n",
              "      <td>0.628440</td>\n",
              "      <td>0.750154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.629100</td>\n",
              "      <td>0.635892</td>\n",
              "      <td>0.635474</td>\n",
              "      <td>0.736516</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 265.4696, 'train_samples_per_second': 71.021, 'train_steps_per_second': 8.882, 'total_flos': 2463244467830784.0, 'train_loss': 0.7092149328437268, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6358919739723206, 'eval_accuracy': 0.635474006116208, 'eval_f1': 0.7365163572060124, 'eval_runtime': 21.6481, 'eval_samples_per_second': 151.052, 'eval_steps_per_second': 18.893, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 42, 'layers_frozen': 9, 'trainable_params_pct': 17.089728635524864, 'accuracy': 0.635474006116208, 'loss': 0.6358919739723206, 'training_time': 21.6481}\n",
            "\n",
            "===== Experiment: gpt2_9_frozen_52 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 9 GPT-2 layers...\n",
            "Froze 9 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 21,266,688 (17.09% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 17.09% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 04:14, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.661400</td>\n",
              "      <td>0.645313</td>\n",
              "      <td>0.628746</td>\n",
              "      <td>0.761024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.624000</td>\n",
              "      <td>0.640688</td>\n",
              "      <td>0.630581</td>\n",
              "      <td>0.737277</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 255.0423, 'train_samples_per_second': 73.925, 'train_steps_per_second': 9.246, 'total_flos': 2463244467830784.0, 'train_loss': 0.6563806250704053, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6406880021095276, 'eval_accuracy': 0.6305810397553517, 'eval_f1': 0.7372770769899957, 'eval_runtime': 21.4903, 'eval_samples_per_second': 152.162, 'eval_steps_per_second': 19.032, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 52, 'layers_frozen': 9, 'trainable_params_pct': 17.089728635524864, 'accuracy': 0.6305810397553517, 'loss': 0.6406880021095276, 'training_time': 21.4903}\n",
            "\n",
            "===== Experiment: gpt2_9_frozen_62 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 9 GPT-2 layers...\n",
            "Froze 9 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 21,266,688 (17.09% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 17.09% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 04:20, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.650700</td>\n",
              "      <td>0.638234</td>\n",
              "      <td>0.638532</td>\n",
              "      <td>0.755684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.611000</td>\n",
              "      <td>0.634570</td>\n",
              "      <td>0.636697</td>\n",
              "      <td>0.723721</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 260.5154, 'train_samples_per_second': 72.372, 'train_steps_per_second': 9.051, 'total_flos': 2463244467830784.0, 'train_loss': 0.6468502817970904, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.63823401927948, 'eval_accuracy': 0.6385321100917432, 'eval_f1': 0.7556841670111616, 'eval_runtime': 21.3948, 'eval_samples_per_second': 152.841, 'eval_steps_per_second': 19.117, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 62, 'layers_frozen': 9, 'trainable_params_pct': 17.089728635524864, 'accuracy': 0.6385321100917432, 'loss': 0.63823401927948, 'training_time': 21.3948}\n",
            "\n",
            "===== Experiment: gpt2_9_frozen_72 =====\n",
            "Loading tokenizer: gpt2\n",
            "Loading model: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f03aad642d44>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing 9 GPT-2 layers...\n",
            "Froze 9 GPT-2 transformer layers\n",
            "Embeddings frozen\n",
            "Trainable parameters: 21,266,688 (17.09% of total)\n",
            "Total parameters: 124,441,344\n",
            "\n",
            "Starting fine-tuning with 17.09% parameters trainable.\n",
            "Tokenizing datasets...\n",
            "Tokenized dataset structure: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 9427\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 3270\n",
            "    })\n",
            "})\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2358' max='2358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2358/2358 04:16, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.654200</td>\n",
              "      <td>0.639355</td>\n",
              "      <td>0.640061</td>\n",
              "      <td>0.762941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.621500</td>\n",
              "      <td>0.631487</td>\n",
              "      <td>0.648318</td>\n",
              "      <td>0.738992</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Results: {'train_runtime': 257.0409, 'train_samples_per_second': 73.35, 'train_steps_per_second': 9.174, 'total_flos': 2463244467830784.0, 'train_loss': 0.6485300557506196, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='409' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [409/409 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.6314868330955505, 'eval_accuracy': 0.6483180428134556, 'eval_f1': 0.7389922832501135, 'eval_runtime': 21.4955, 'eval_samples_per_second': 152.125, 'eval_steps_per_second': 19.027, 'epoch': 2.0}\n",
            "Experiment results: {'model': 'gpt2', 'seeds': 72, 'layers_frozen': 9, 'trainable_params_pct': 17.089728635524864, 'accuracy': 0.6483180428134556, 'loss': 0.6314868330955505, 'training_time': 21.4955}\n",
            "\n",
            "===== Summary of All Experiments =====\n",
            "   model  seeds  layers_frozen  trainable_params_pct  accuracy      loss  \\\n",
            "0   gpt2     42              0             68.351509  0.685015  0.606364   \n",
            "1   gpt2     52              0             68.351509  0.674006  0.616565   \n",
            "2   gpt2     62              0             68.351509  0.669113  0.613543   \n",
            "3   gpt2     72              0             68.351509  0.677982  0.605362   \n",
            "4   gpt2     42              3             51.264249  0.683792  0.611104   \n",
            "5   gpt2     52              3             51.264249  0.687768  0.616664   \n",
            "6   gpt2     62              3             51.264249  0.677064  0.608307   \n",
            "7   gpt2     72              3             51.264249  0.674312  0.613144   \n",
            "8   gpt2     42              6             34.176989  0.666972  0.625922   \n",
            "9   gpt2     52              6             34.176989  0.671865  0.623879   \n",
            "10  gpt2     62              6             34.176989  0.680122  0.611707   \n",
            "11  gpt2     72              6             34.176989  0.675229  0.614346   \n",
            "12  gpt2     42              9             17.089729  0.635474  0.635892   \n",
            "13  gpt2     52              9             17.089729  0.630581  0.640688   \n",
            "14  gpt2     62              9             17.089729  0.638532  0.638234   \n",
            "15  gpt2     72              9             17.089729  0.648318  0.631487   \n",
            "\n",
            "    training_time  \n",
            "0         21.3123  \n",
            "1         21.2913  \n",
            "2         21.3820  \n",
            "3         21.4114  \n",
            "4         21.4242  \n",
            "5         21.5062  \n",
            "6         21.3394  \n",
            "7         21.5231  \n",
            "8         21.3244  \n",
            "9         21.2947  \n",
            "10        21.4785  \n",
            "11        21.2906  \n",
            "12        21.6481  \n",
            "13        21.4903  \n",
            "14        21.3948  \n",
            "15        21.4955  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== All experiments completed =====\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import os\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}